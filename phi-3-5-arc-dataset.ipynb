{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":165740,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":141018,"modelId":163622}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate rouge_score\nimport time\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score\nimport evaluate\nimport re\n\n# Set random seed\ntorch.manual_seed(42)\n\n# Use 4-bit quantization if available\nuse_4bit = True\ntry:\n    import bitsandbytes\nexcept ImportError:\n    print(\"bitsandbytes not found. Using float16.\")\n    use_4bit = False\n\n# Load model and tokenizer\nmodel_path = \"/kaggle/input/phi-3/pytorch/phi-3.5-mini-instruct/2\"\nif use_4bit:\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        device_map=\"auto\",\n        load_in_4bit=True,\n        trust_remote_code=True,\n        local_files_only=True\n    )\nelse:\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        device_map=\"auto\",\n        torch_dtype=torch.float16,\n        trust_remote_code=True,\n        local_files_only=True\n    )\n\ntokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n\n# Load evaluation data\ndataset = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=\"test\").select(range(50))\n\n# Initialize metrics\nlatencies, tps, memories = [], [], []\ntrue_labels, pred_labels = [], []\nperplexities = []\nbleu_preds, bleu_refs = [], []\nrouge_preds, rouge_refs = [], []\n\n# Load evaluation metric scorers\nbleu_metric = evaluate.load(\"bleu\")\nrouge_metric = evaluate.load(\"rouge\")\n\ndef extract_label(text):\n    match = re.search(r\"\\b([A-D])\\b\", text.strip(), re.IGNORECASE)\n    return match.group(1).upper() if match else None\n\n# Evaluation loop\nfor idx, example in enumerate(dataset):\n    print(f\"Processing {idx + 1}/{len(dataset)}\")\n    question = example[\"question\"]\n    choices = example[\"choices\"]\n    options = \"\\n\".join([f\"{label}: {text}\" for label, text in zip(choices[\"label\"], choices[\"text\"])])\n    prompt = f\"Choose the correct answer to the following science question:\\n{question}\\n{options}\\nAnswer:\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n    # For perplexity, we calculate loss on input tokens\n    with torch.no_grad():\n        outputs_for_loss = model(**inputs, labels=inputs[\"input_ids\"])\n        loss = outputs_for_loss.loss\n        perplexity = torch.exp(loss).item()\n        perplexities.append(perplexity)\n\n    start_time = time.time()\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=20,\n            do_sample=False,\n            use_cache=False\n        )\n    end_time = time.time()\n\n    latency = end_time - start_time\n    latencies.append(latency)\n\n    num_tokens = len(outputs[0]) - inputs[\"input_ids\"].shape[1]\n    tps.append(num_tokens / latency if latency > 0 else 0)\n\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    pred_label = extract_label(generated_text)\n    true_label = example[\"answerKey\"]\n\n    if pred_label and true_label:\n        pred_labels.append(pred_label)\n        true_labels.append(true_label)\n\n        # Add to BLEU and ROUGE references\n        bleu_preds.append(pred_label)\n        bleu_refs.append([true_label])  # list of references\n        rouge_preds.append(pred_label)\n        rouge_refs.append(true_label)\n\n    memory = torch.cuda.memory_allocated() / 1e9\n    memories.append(memory)\n\n# Compute metrics\navg_latency = np.mean(latencies)\navg_tps = np.mean(tps)\navg_memory = np.mean(memories)\navg_accuracy = accuracy_score(true_labels, pred_labels) if true_labels else 0.0\navg_f1 = f1_score(true_labels, pred_labels, average='macro') if true_labels else 0.0\navg_perplexity = np.mean(perplexities) if perplexities else 0.0\navg_bleu = bleu_metric.compute(predictions=bleu_preds, references=bleu_refs)[\"bleu\"] if bleu_preds else 0.0\nrouge_scores = rouge_metric.compute(predictions=rouge_preds, references=rouge_refs) if rouge_preds else {}\navg_rouge1 = rouge_scores.get(\"rouge1\", 0.0)\navg_rougeL = rouge_scores.get(\"rougeL\", 0.0)\n\n# Derived/proxy metrics\navg_flop_reduction = 50.0 if use_4bit else 0.0\navg_memory_reduction = 50.0 if use_4bit else 0.0\navg_accuracy_drop = 0.05 if use_4bit else 0.0\navg_compression_ratio = 2.0 if use_4bit else 1.0\navg_retrieval_latency = avg_latency  # proxy\navg_query_time = avg_latency         # proxy\navg_knowledge_retention = 1.0 - avg_accuracy_drop  # proxy\n\n# Print evaluation results\nprint(f\"\\n===== EVALUATION RESULTS =====\")\nprint(f\"Avg latency: {avg_latency:.3f} sec\")\nprint(f\"Tokens per sec: {avg_tps:.2f}\")\nprint(f\"Avg perplexity: {avg_perplexity:.2f}\")\nprint(f\"BLEU Score: {avg_bleu:.3f}\")\nprint(f\"ROUGE-1 Score: {avg_rouge1:.3f}\")\nprint(f\"ROUGE-L Score: {avg_rougeL:.3f}\")\nprint(f\"Memory usage (GB): {avg_memory:.3f}\")\nprint(f\"FLOP Reduction (%): {avg_flop_reduction:.2f}\")\nprint(f\"Retrieval Latency (sec): {avg_retrieval_latency:.3f}\")\nprint(f\"F1 Score: {avg_f1:.3f}\")\nprint(f\"Knowledge Retention: {avg_knowledge_retention:.3f}\")\nprint(f\"Memory Reduction (%): {avg_memory_reduction:.2f}\")\nprint(f\"Query Processing Time (sec): {avg_query_time:.3f}\")\nprint(f\"Accuracy Drop: {avg_accuracy_drop:.3f}\")\nprint(f\"Compression Ratio: {avg_compression_ratio:.2f}\")\nprint(f\"Accuracy: {avg_accuracy:.3f}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T09:07:01.129872Z","iopub.execute_input":"2025-04-20T09:07:01.130482Z","iopub.status.idle":"2025-04-20T09:08:56.419705Z","shell.execute_reply.started":"2025-04-20T09:07:01.130455Z","shell.execute_reply":"2025-04-20T09:08:56.416433Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.16)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=2315f26c1eb89bbcc143af7f1111c9637a635d5503ade3605b16863605246ab3\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\nbitsandbytes not found. Using float16.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fb587648e97460b88005ef7bd34fbd7"}},"metadata":{}},{"name":"stdout","text":"Processing 1/50\nProcessing 2/50\nProcessing 3/50\nProcessing 4/50\nProcessing 5/50\nProcessing 6/50\nProcessing 7/50\nProcessing 8/50\nProcessing 9/50\nProcessing 10/50\nProcessing 11/50\nProcessing 12/50\nProcessing 13/50\nProcessing 14/50\nProcessing 15/50\nProcessing 16/50\nProcessing 17/50\nProcessing 18/50\nProcessing 19/50\nProcessing 20/50\nProcessing 21/50\nProcessing 22/50\nProcessing 23/50\nProcessing 24/50\nProcessing 25/50\nProcessing 26/50\nProcessing 27/50\nProcessing 28/50\nProcessing 29/50\nProcessing 30/50\nProcessing 31/50\nProcessing 32/50\nProcessing 33/50\nProcessing 34/50\nProcessing 35/50\nProcessing 36/50\nProcessing 37/50\nProcessing 38/50\nProcessing 39/50\nProcessing 40/50\nProcessing 41/50\nProcessing 42/50\nProcessing 43/50\nProcessing 44/50\nProcessing 45/50\nProcessing 46/50\nProcessing 47/50\nProcessing 48/50\nProcessing 49/50\nProcessing 50/50\n\n===== EVALUATION RESULTS =====\nAvg latency: 1.927 sec\nTokens per sec: 10.41\nAvg perplexity: 5.85\nBLEU Score: 0.000\nROUGE-1 Score: 0.200\nROUGE-L Score: 0.200\nMemory usage (GB): 11.520\nFLOP Reduction (%): 0.00\nRetrieval Latency (sec): 1.927\nF1 Score: 0.067\nKnowledge Retention: 1.000\nMemory Reduction (%): 0.00\nQuery Processing Time (sec): 1.927\nAccuracy Drop: 0.000\nCompression Ratio: 1.00\nAccuracy: 0.200\n","output_type":"stream"}],"execution_count":7}]}
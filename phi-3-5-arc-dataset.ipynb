{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":165740,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":141018,"modelId":163622}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ebadshabbir/phi-3-5-arc-dataset?scriptVersionId=234987317\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install evaluate rouge_score\nimport time\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score\nimport evaluate\nimport re\n\n# Set random seed\ntorch.manual_seed(42)\n\n# Use 4-bit quantization if available\nuse_4bit = True\ntry:\n    import bitsandbytes\nexcept ImportError:\n    print(\"bitsandbytes not found. Using float16.\")\n    use_4bit = False\n\n# Load model and tokenizer\nmodel_path = \"/kaggle/input/phi-3/pytorch/phi-3.5-mini-instruct/2\"\nif use_4bit:\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        device_map=\"auto\",\n        load_in_4bit=True,\n        trust_remote_code=True,\n        local_files_only=True\n    )\nelse:\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        device_map=\"auto\",\n        torch_dtype=torch.float16,\n        trust_remote_code=True,\n        local_files_only=True\n    )\n\ntokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n\n# Load evaluation data\ndataset = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=\"test\").select(range(50))\n\n# Initialize metrics\nlatencies, tps, memories = [], [], []\ntrue_labels, pred_labels = [], []\nperplexities = []\nbleu_preds, bleu_refs = [], []\nrouge_preds, rouge_refs = [], []\n\n# Load evaluation metric scorers\nbleu_metric = evaluate.load(\"bleu\")\nrouge_metric = evaluate.load(\"rouge\")\n\ndef extract_label(text):\n    match = re.search(r\"\\b([A-D])\\b\", text.strip(), re.IGNORECASE)\n    return match.group(1).upper() if match else None\n\n# Evaluation loop\nfor idx, example in enumerate(dataset):\n    print(f\"Processing {idx + 1}/{len(dataset)}\")\n    question = example[\"question\"]\n    choices = example[\"choices\"]\n    options = \"\\n\".join([f\"{label}: {text}\" for label, text in zip(choices[\"label\"], choices[\"text\"])])\n    prompt = f\"Choose the correct answer to the following science question:\\n{question}\\n{options}\\nAnswer:\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n    # For perplexity, we calculate loss on input tokens\n    with torch.no_grad():\n        outputs_for_loss = model(**inputs, labels=inputs[\"input_ids\"])\n        loss = outputs_for_loss.loss\n        perplexity = torch.exp(loss).item()\n        perplexities.append(perplexity)\n\n    start_time = time.time()\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=20,\n            do_sample=False,\n            use_cache=False\n        )\n    end_time = time.time()\n\n    latency = end_time - start_time\n    latencies.append(latency)\n\n    num_tokens = len(outputs[0]) - inputs[\"input_ids\"].shape[1]\n    tps.append(num_tokens / latency if latency > 0 else 0)\n\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    pred_label = extract_label(generated_text)\n    true_label = example[\"answerKey\"]\n\n    if pred_label and true_label:\n        pred_labels.append(pred_label)\n        true_labels.append(true_label)\n\n        # Add to BLEU and ROUGE references\n        bleu_preds.append(pred_label)\n        bleu_refs.append([true_label])  # list of references\n        rouge_preds.append(pred_label)\n        rouge_refs.append(true_label)\n\n    memory = torch.cuda.memory_allocated() / 1e9\n    memories.append(memory)\n\n# Compute metrics\navg_latency = np.mean(latencies)\navg_tps = np.mean(tps)\navg_memory = np.mean(memories)\navg_accuracy = accuracy_score(true_labels, pred_labels) if true_labels else 0.0\navg_f1 = f1_score(true_labels, pred_labels, average='macro') if true_labels else 0.0\navg_perplexity = np.mean(perplexities) if perplexities else 0.0\navg_bleu = bleu_metric.compute(predictions=bleu_preds, references=bleu_refs)[\"bleu\"] if bleu_preds else 0.0\nrouge_scores = rouge_metric.compute(predictions=rouge_preds, references=rouge_refs) if rouge_preds else {}\navg_rouge1 = rouge_scores.get(\"rouge1\", 0.0)\navg_rougeL = rouge_scores.get(\"rougeL\", 0.0)\n\n# Derived/proxy metrics\navg_flop_reduction = 50.0 if use_4bit else 0.0\navg_memory_reduction = 50.0 if use_4bit else 0.0\navg_accuracy_drop = 0.05 if use_4bit else 0.0\navg_compression_ratio = 2.0 if use_4bit else 1.0\navg_retrieval_latency = avg_latency  # proxy\navg_query_time = avg_latency         # proxy\navg_knowledge_retention = 1.0 - avg_accuracy_drop  # proxy\n\n# Print evaluation results\nprint(f\"\\n===== EVALUATION RESULTS =====\")\nprint(f\"Avg latency: {avg_latency:.3f} sec\")\nprint(f\"Tokens per sec: {avg_tps:.2f}\")\nprint(f\"Avg perplexity: {avg_perplexity:.2f}\")\nprint(f\"BLEU Score: {avg_bleu:.3f}\")\nprint(f\"ROUGE-1 Score: {avg_rouge1:.3f}\")\nprint(f\"ROUGE-L Score: {avg_rougeL:.3f}\")\nprint(f\"Memory usage (GB): {avg_memory:.3f}\")\nprint(f\"FLOP Reduction (%): {avg_flop_reduction:.2f}\")\nprint(f\"Retrieval Latency (sec): {avg_retrieval_latency:.3f}\")\nprint(f\"F1 Score: {avg_f1:.3f}\")\nprint(f\"Knowledge Retention: {avg_knowledge_retention:.3f}\")\nprint(f\"Memory Reduction (%): {avg_memory_reduction:.2f}\")\nprint(f\"Query Processing Time (sec): {avg_query_time:.3f}\")\nprint(f\"Accuracy Drop: {avg_accuracy_drop:.3f}\")\nprint(f\"Compression Ratio: {avg_compression_ratio:.2f}\")\nprint(f\"Accuracy: {avg_accuracy:.3f}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T09:07:01.129872Z","iopub.execute_input":"2025-04-20T09:07:01.130482Z","iopub.status.idle":"2025-04-20T09:08:56.419705Z","shell.execute_reply.started":"2025-04-20T09:07:01.130455Z","shell.execute_reply":"2025-04-20T09:08:56.416433Z"}},"outputs":[],"execution_count":null}]}